{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:32.647489Z",
     "iopub.status.busy": "2021-06-14T15:27:32.646933Z",
     "iopub.status.idle": "2021-06-14T15:27:33.381796Z",
     "shell.execute_reply": "2021-06-14T15:27:33.380929Z",
     "shell.execute_reply.started": "2021-06-14T15:27:32.64745Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82f6c1ea"
   },
   "source": [
    "### Import Relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.384267Z",
     "iopub.status.busy": "2021-06-14T15:27:33.383619Z",
     "iopub.status.idle": "2021-06-14T15:27:33.390297Z",
     "shell.execute_reply": "2021-06-14T15:27:33.389287Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.384212Z"
    },
    "id": "e3b7e260"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f980309"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.393287Z",
     "iopub.status.busy": "2021-06-14T15:27:33.392778Z",
     "iopub.status.idle": "2021-06-14T15:27:33.40232Z",
     "shell.execute_reply": "2021-06-14T15:27:33.401133Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.393234Z"
    },
    "id": "0019da94"
   },
   "outputs": [],
   "source": [
    "Path = [\"../input/div2k/DIV2K_train_HR/DIV2K_train_HR/*.png\",\n",
    "        \"../input/llsrdatasets/My_datasets/Train_data/BSDS200/*.png\"]\n",
    "height = 224\n",
    "width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.404742Z",
     "iopub.status.busy": "2021-06-14T15:27:33.404074Z",
     "iopub.status.idle": "2021-06-14T15:27:33.414598Z",
     "shell.execute_reply": "2021-06-14T15:27:33.413638Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.404687Z"
    },
    "id": "402acac5"
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img,channels=3)\n",
    "    img = tf.cast(img,tf.float32)\n",
    "    img = tf.image.random_crop(img, [height,width,3])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.416633Z",
     "iopub.status.busy": "2021-06-14T15:27:33.416215Z",
     "iopub.status.idle": "2021-06-14T15:27:33.426374Z",
     "shell.execute_reply": "2021-06-14T15:27:33.425439Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.416589Z"
    },
    "id": "19185c5e"
   },
   "outputs": [],
   "source": [
    "def reshape_normalize(img):\n",
    "    #img = tf.image.resize(img,[height,width],method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    img = img/127 - 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.427857Z",
     "iopub.status.busy": "2021-06-14T15:27:33.427562Z",
     "iopub.status.idle": "2021-06-14T15:27:33.437805Z",
     "shell.execute_reply": "2021-06-14T15:27:33.43656Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.427829Z"
    },
    "id": "a20869cd"
   },
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    hrimg= read_file(path)\n",
    "    hrimg = reshape_normalize(hrimg)\n",
    "    return hrimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.439529Z",
     "iopub.status.busy": "2021-06-14T15:27:33.439223Z",
     "iopub.status.idle": "2021-06-14T15:27:33.813364Z",
     "shell.execute_reply": "2021-06-14T15:27:33.81242Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.4395Z"
    },
    "id": "9d93645c"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(Path)\n",
    "dataset = dataset.map(load_img)\n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.816814Z",
     "iopub.status.busy": "2021-06-14T15:27:33.816166Z",
     "iopub.status.idle": "2021-06-14T15:27:33.826913Z",
     "shell.execute_reply": "2021-06-14T15:27:33.825716Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.816763Z"
    },
    "id": "cc155f9b"
   },
   "outputs": [],
   "source": [
    "def dense_block(input):\n",
    "    initializer = tf.random_normal_initializer(0.0,0.02)\n",
    "    c1 = tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 1, padding = 'same',kernel_initializer = initializer)(input)\n",
    "    a1 = tf.keras.layers.LeakyReLU()(c1)\n",
    "    a1 = tf.keras.layers.Concatenate()([input,a1])\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 1, padding = 'same',kernel_initializer = initializer)(a1)\n",
    "    a2 = tf.keras.layers.LeakyReLU()(c2)\n",
    "    a2 = tf.keras.layers.Concatenate()([input,a1,a2])\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 1, padding = 'same',kernel_initializer = initializer)(a2)\n",
    "    a3 = tf.keras.layers.LeakyReLU()(c3)\n",
    "    a3 = tf.keras.layers.Concatenate()([input,a1,a2,a3])\n",
    "    \n",
    "    c4 = tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 1, padding = 'same',kernel_initializer = initializer)(a3)\n",
    "    a4 = tf.keras.layers.LeakyReLU()(c4)\n",
    "    a4 = tf.keras.layers.Concatenate()([input,a1,a2,a3,a4])\n",
    "    \n",
    "    c5 = tf.keras.layers.Conv2D(64, kernel_size = 3, strides = 1, padding = 'same',kernel_initializer = initializer)(a4)\n",
    "    c5 = c5*0.2\n",
    "    layer = tf.keras.layers.Add()([c5,input])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.830072Z",
     "iopub.status.busy": "2021-06-14T15:27:33.829198Z",
     "iopub.status.idle": "2021-06-14T15:27:33.842243Z",
     "shell.execute_reply": "2021-06-14T15:27:33.840832Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.830019Z"
    },
    "id": "0ee78d0b"
   },
   "outputs": [],
   "source": [
    "def rddb_block(input):\n",
    "    l = dense_block(input)\n",
    "    l = dense_block(l)\n",
    "    l = dense_block(l)\n",
    "    l = l*0.2\n",
    "    l = tf.keras.layers.Add()([input,l])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:33.844276Z",
     "iopub.status.busy": "2021-06-14T15:27:33.843838Z",
     "iopub.status.idle": "2021-06-14T15:27:38.536886Z",
     "shell.execute_reply": "2021-06-14T15:27:38.535878Z",
     "shell.execute_reply.started": "2021-06-14T15:27:33.844237Z"
    },
    "id": "5392f380"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    initializer = tf.random_normal_initializer(0.0,0.02)\n",
    "    input = tf.keras.layers.Input(shape=[None, None, 3])\n",
    "    m = tf.keras.layers.Conv2D(64,kernel_size=3, strides = 1,kernel_initializer = initializer,padding = 'same')(input)\n",
    "    model = rddb_block(m)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = rddb_block(model)\n",
    "    model = tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 1, padding = 'same')(model)\n",
    "    model = tf.keras.layers.Concatenate()([m,model])\n",
    "    model = tf.nn.depth_to_space(model,2)\n",
    "    model = tf.keras.layers.Conv2D(128, kernel_size = 3, strides = 1,padding = 'same')(model)\n",
    "    model = tf.nn.depth_to_space(model,2)\n",
    "    #model = tf.keras.layers.UpSampling2D(size = 2)(model)\n",
    "    model = tf.keras.layers.Conv2D(256,kernel_size = 3, strides = 1,padding = 'same')(model)\n",
    "    model = tf.keras.layers.Conv2D(3,kernel_size = 9,strides = 1,kernel_initializer = initializer,padding = 'same')(model)\n",
    "    model = tf.keras.Model(inputs = input,outputs = model)\n",
    "    return model\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:38.539057Z",
     "iopub.status.busy": "2021-06-14T15:27:38.538575Z",
     "iopub.status.idle": "2021-06-14T15:27:38.554527Z",
     "shell.execute_reply": "2021-06-14T15:27:38.553128Z",
     "shell.execute_reply.started": "2021-06-14T15:27:38.539002Z"
    },
    "id": "561c5afe"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    initializer = tf.random_normal_initializer(0.0,0.02)\n",
    "    inp = tf.keras.layers.Input(shape = [height,width,3])\n",
    "    tar = tf.keras.layers.Input(shape=[height,width,3])\n",
    "    input = tf.keras.layers.Concatenate()([inp,tar])\n",
    "    input = tf.keras.layers.GaussianNoise(0.2)(input)\n",
    "    model = tf.keras.layers.Conv2D(64, kernel_size = 3,strides = 1, kernel_initializer = initializer,padding = 'same')(input)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(64, kernel_size = 3,strides = 2,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(128, kernel_size = 3,strides = 1,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    #model = tf.keras.layers.GaussianNoise(0.03)(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(128, kernel_size = 3,strides = 2,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(256, kernel_size = 3,strides = 1,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    #model = tf.keras.layers.GaussianNoise(0.1)(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(256, kernel_size = 3,strides = 2,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Conv2D(512, kernel_size = 3,strides = 2,kernel_initializer = initializer, padding = 'same')(model)\n",
    "    model = tf.keras.layers.BatchNormalization()(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    \n",
    "    model = tf.keras.layers.Dense(1024)(model)\n",
    "    model = tf.keras.layers.LeakyReLU()(model)\n",
    "    model = tf.keras.layers.Dense(1)(model)\n",
    "    model = tf.keras.layers.Activation('sigmoid')(model)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [inp,tar], outputs = model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:38.556621Z",
     "iopub.status.busy": "2021-06-14T15:27:38.556147Z",
     "iopub.status.idle": "2021-06-14T15:27:38.802356Z",
     "shell.execute_reply": "2021-06-14T15:27:38.801377Z",
     "shell.execute_reply.started": "2021-06-14T15:27:38.556583Z"
    },
    "id": "188cca6a",
    "outputId": "e6f62c3d-a31e-48b5-85a9-10df8e219723"
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8ccc896"
   },
   "source": [
    "### Downloading the VGG19 classification model pre-trained on the Imagenet dataset (Used for the perceptual loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:38.803848Z",
     "iopub.status.busy": "2021-06-14T15:27:38.803565Z",
     "iopub.status.idle": "2021-06-14T15:27:46.728054Z",
     "shell.execute_reply": "2021-06-14T15:27:46.726792Z",
     "shell.execute_reply.started": "2021-06-14T15:27:38.803818Z"
    }
   },
   "outputs": [],
   "source": [
    "v = VGG19(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping the VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:46.731993Z",
     "iopub.status.busy": "2021-06-14T15:27:46.731657Z",
     "iopub.status.idle": "2021-06-14T15:27:46.75229Z",
     "shell.execute_reply": "2021-06-14T15:27:46.748244Z",
     "shell.execute_reply.started": "2021-06-14T15:27:46.731945Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg = tf.keras.models.Model(inputs = v.input, outputs = v.get_layer('block4_conv4').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:46.754254Z",
     "iopub.status.busy": "2021-06-14T15:27:46.753823Z",
     "iopub.status.idle": "2021-06-14T15:27:46.76361Z",
     "shell.execute_reply": "2021-06-14T15:27:46.762629Z",
     "shell.execute_reply.started": "2021-06-14T15:27:46.754213Z"
    },
    "id": "kix5IW94WhLa"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_output_rf,disc_output_fr,gen_output,hr_image):\n",
    "    lgra = (tf.keras.losses.binary_crossentropy(tf.ones_like(disc_output_fr),disc_output_fr)+\n",
    "            tf.keras.losses.binary_crossentropy(tf.zeros_like(disc_output_rf),disc_output_rf))\n",
    "    l1 = tf.reduce_mean(tf.abs(hr_image - gen_output))\n",
    "    gen_feature = vgg(preprocess_input(hr_image))\n",
    "    original_feature = vgg(preprocess_input(gen_output))\n",
    "    percept_loss = tf.reduce_mean(tf.losses.mean_squared_error(gen_feature,original_feature))\n",
    "    total_loss = percept_loss + (5e-3)*lgra + (1e-2)*l1\n",
    "    return total_loss , l1, percept_loss,lgra\n",
    "\n",
    "def discriminator_loss(disc_output_rf,disc_output_fr):\n",
    "    ldra = (tf.keras.losses.binary_crossentropy(tf.ones_like(disc_output_rf),disc_output_rf)+\n",
    "            tf.keras.losses.binary_crossentropy(tf.zeros_like(disc_output_fr),disc_output_fr))\n",
    "    return ldra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the optimizers for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:46.76535Z",
     "iopub.status.busy": "2021-06-14T15:27:46.764857Z",
     "iopub.status.idle": "2021-06-14T15:27:46.783119Z",
     "shell.execute_reply": "2021-06-14T15:27:46.781694Z",
     "shell.execute_reply.started": "2021-06-14T15:27:46.765302Z"
    },
    "id": "91fc312e"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1.25e-5,beta_1=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(5e-7,beta_1=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:27:46.786117Z",
     "iopub.status.busy": "2021-06-14T15:27:46.785242Z",
     "iopub.status.idle": "2021-06-14T15:28:13.902947Z",
     "shell.execute_reply": "2021-06-14T15:28:13.90177Z",
     "shell.execute_reply.started": "2021-06-14T15:27:46.785964Z"
    }
   },
   "outputs": [],
   "source": [
    "#generator = tf.keras.models.load_model(\"../input/image-super-resolution-gan/generator24\")\n",
    "#discriminator = tf.keras.models.load_model(\"../input/image-super-resolution-gan/discriminator24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b59db92"
   },
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:28:13.905326Z",
     "iopub.status.busy": "2021-06-14T15:28:13.904876Z",
     "iopub.status.idle": "2021-06-14T15:28:13.913437Z",
     "shell.execute_reply": "2021-06-14T15:28:13.912601Z",
     "shell.execute_reply.started": "2021-06-14T15:28:13.905267Z"
    },
    "id": "c60fcd47"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(target,input_image,epoch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image,training = True)\n",
    "        psnr = tf.image.psnr(gen_output, target, max_val=1.0)\n",
    "        disc_output_fr = discriminator([gen_output,target],training = True)\n",
    "        disc_output_rf = discriminator([target,gen_output],training = True)\n",
    "        gen_total_loss, l1, percept_loss ,lgra= generator_loss(disc_output_rf,disc_output_fr,gen_output,target)\n",
    "        disc_loss = discriminator_loss(disc_output_rf,disc_output_fr)\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,discriminator.trainable_variables))\n",
    "    return gen_total_loss , l1, percept_loss,disc_loss,psnr,lgra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:28:13.915444Z",
     "iopub.status.busy": "2021-06-14T15:28:13.914767Z",
     "iopub.status.idle": "2021-06-14T15:28:13.935379Z",
     "shell.execute_reply": "2021-06-14T15:28:13.934319Z",
     "shell.execute_reply.started": "2021-06-14T15:28:13.915401Z"
    },
    "id": "c58feaf4"
   },
   "outputs": [],
   "source": [
    "def fit(dataset,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        sum_gloss = 0\n",
    "        sum_dloss = 0\n",
    "        sum_psnr = 0\n",
    "        sum_l1 = 0\n",
    "        sum_lgra = 0\n",
    "        sum_perceptl = 0\n",
    "        for hr_img in dataset.take(1):\n",
    "            lr_img = tf.image.resize(hr_img,[width//4,height//4])\n",
    "            pre_img = generator(lr_img, training = True)\n",
    "            plt.figure(figsize = (32,32))\n",
    "            psnr = tf.image.psnr(pre_img, hr_img, max_val=1.0)\n",
    "            tf.print(\"PSNR = \",psnr)\n",
    "            display_list= [lr_img[0],hr_img[0],pre_img[0]]\n",
    "            title = [\"4x Downscaled\",\"Original HR\",\"Upscaled\"]\n",
    "            plt.subplot(1,3,0+1)\n",
    "            plt.title(title[0],fontsize = 25)\n",
    "            plt.imshow(display_list[0]*0.5+0.5)\n",
    "            plt.subplot(1,3,1+1)\n",
    "            plt.title(title[1],fontsize = 25)\n",
    "            plt.imshow(display_list[1]*0.5+0.5)\n",
    "            plt.subplot(1,3,2+1)\n",
    "            plt.title(title[2],fontsize = 25)\n",
    "            plt.imshow(display_list[2]*0.5+0.5)\n",
    "            plt.show()\n",
    "        print(\"Epoch : \",epoch)\n",
    "        for n,hr_image in dataset.enumerate():\n",
    "            lr_image = tf.image.resize(hr_image,[width//4,height//4],method = 'bicubic')\n",
    "            if n%20==0:\n",
    "                gen_output = generator(lr_image,training = False)\n",
    "                display_list= [lr_image[0],hr_image[0],gen_output[0]]\n",
    "                title = [\"4x Downscaled\",\"Original HR\",\"Upscaled\"]\n",
    "                plt.figure(figsize = (32,32))\n",
    "                plt.subplot(1,3,0+1)\n",
    "                plt.title(title[0],fontsize = 25)\n",
    "                plt.imshow(display_list[0]*0.5+0.5)\n",
    "                plt.subplot(1,3,1+1)\n",
    "                plt.title(title[1],fontsize = 25)\n",
    "                plt.imshow(display_list[1]*0.5+0.5)\n",
    "                plt.subplot(1,3,2+1)\n",
    "                plt.title(title[2],fontsize = 25)\n",
    "                plt.imshow(display_list[2]*0.5+0.5)\n",
    "                plt.show()\n",
    "            gen_total_loss, l1, percept_loss, disc_loss, psnr,lgra= train_step(hr_image,lr_image,epoch)\n",
    "            sum_gloss += gen_total_loss\n",
    "            sum_dloss += disc_loss\n",
    "            sum_psnr += psnr\n",
    "            sum_l1 += l1\n",
    "            sum_lgra += lgra\n",
    "            sum_perceptl += percept_loss\n",
    "        print(\"Perceptual :\")\n",
    "        print(tf.reduce_mean(sum_perceptl))\n",
    "        print(\"PSNR :\")\n",
    "        print(tf.reduce_mean(sum_psnr))\n",
    "        print(\"L1 :\")\n",
    "        print(sum_l1)\n",
    "        print(\"Gloss :\")\n",
    "        print(tf.reduce_mean(sum_gloss))\n",
    "        print(\"Dloss :\")\n",
    "        print(tf.reduce_mean(sum_dloss))\n",
    "        print(\"Lgra :\")\n",
    "        print(tf.reduce_mean(sum_lgra))\n",
    "        print()\n",
    "        generator.save((\"generator\"+str(epoch)))\n",
    "        discriminator.save((\"discriminator\"+str(epoch)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T15:28:13.937164Z",
     "iopub.status.busy": "2021-06-14T15:28:13.936821Z"
    },
    "id": "b187352a",
    "outputId": "e090ee5c-9562-4305-83eb-59c09d5d01f1"
   },
   "outputs": [],
   "source": [
    "fit(dataset,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0vXLDa6zk_Q"
   },
   "outputs": [],
   "source": [
    "#generator.save(\"generator final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator.save(\"discriminator final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
